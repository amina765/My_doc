{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4bcc42c-88d9-4e8e-b444-0cf5150d42e0",
   "metadata": {},
   "source": [
    "1.ENABLE SPARK LOGGING :\n",
    "   helps reduce or increase log verbosity for better debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a28175-1de2-4aa2-8085-6a73705875a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\") #ALL,DEBUG,INFO,WARNING,ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f4070-e9d0-4c82-90ba-bacd0c22bc8e",
   "metadata": {},
   "source": [
    "2.CHECK SPARKCONFIGURATION AT RUNTIME:\n",
    "      useful to verify memory,cores,shuffle config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df8e80-14a6-4d95-a502-5910f28f7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbba98b-2690-40c8-8bd5-c1d3b920a729",
   "metadata": {},
   "source": [
    "3.EXECUTOR MEMORY DEBUG:\n",
    "      used when facing outofmemory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c10e1-72af-4f54-a914-6a5dfcddd786",
   "metadata": {},
   "outputs": [],
   "source": [
    "--conf spark.executor.memory=2g\n",
    "--conf spark.driver.memory=1g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a57374-b4a1-4120-b039-7fb339fabdd6",
   "metadata": {},
   "source": [
    "4.SHUFFLE DEBUGGING-TOO MUCH SHUFFLE:\n",
    "      avoids default small partitions that cause skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3c769a-479c-43c9-a10f-7d41bb0b4ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.repartition(100,\"key\") #before groupBy or join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92395ab-5b02-48c5-b99d-7e534f35609d",
   "metadata": {},
   "source": [
    "5.HANDLING SKEWED JOIN KEYS :\n",
    "      fixes long tasks due to skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d573093-b988-4430-85c2-0184165016b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "result=big_df.join(broadcast(small_df),\"key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff072f3f-ee19-4149-af24-de41b7212da7",
   "metadata": {},
   "source": [
    "6.CATCH READ FAILURES :\n",
    "       good for detecting bad paths or file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ea423-5500-4b35-9f1f-800f0f6a157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  df=spark.read.csv(\"path/does/not/exist\")\n",
    "except Exception as e:\n",
    "  print(\"Failed to read:\",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179ce8d-61c7-4ce8-9999-45130314b7a6",
   "metadata": {},
   "source": [
    "7.MEMORY USAGE VIA EXECUTORS TAB :\n",
    "      Diagnoses executor memory issues visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da49301-15bf-4be1-a9b1-d2eaf8c60b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Use Spark UI at localhost:4040>Executors tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b5eee-5365-41d0-94b9-b836db005fc5",
   "metadata": {},
   "source": [
    "8.MONITOR JOB EXECUTION TIME\n",
    "useful to measure slow operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff90de3-a86e-479c-80de-ac9fadab99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start=time()\n",
    "df.count()\n",
    "print(\"Execution Time:\",time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ac143-17e4-4ad7-9b18-e63f35a7b976",
   "metadata": {},
   "source": [
    "9.USE.PRESIST() WISELY :\n",
    "      Avoid caching raw or large unfiltered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55891c01-3dfc-46c8-bfb1-acc6f3d4a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.filter(\"status=\"active\").persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf32801-908d-4708-9ce3-e4b32d815f61",
   "metadata": {},
   "source": [
    "10.ANALYZE TASK DURATION :\n",
    "      useful to detect long-running tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f69ce7-b603-4725-a956-c316aa2b53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "View task skew in spark UI>stages>Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ea1c8-0228-4088-a866-ff23c0ff29b3",
   "metadata": {},
   "source": [
    "11.AVOID EXPLODING MEMORY IN COLLECT:\n",
    "       prevents memory issue on the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b67942-fdf6-4948-b577-a135fef98b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bad\n",
    "all_rows=df.collect()\n",
    "#Better\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2a608c-736e-43bf-946b-b16f9fe4d857",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "12.CHECK NUMBER OF PARTITIONS :\n",
    "        too few partitions cause bottlenecks.Too many = overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e705e6d1-2ff0-490a-91ac-f8035cca02da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f68786-5abc-4748-bc6b-374ac2b442ee",
   "metadata": {},
   "source": [
    "13.DEBUG JOIN TYPE ISSUES :\n",
    "       check for broadcast hint or sort-merge join issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69c81d-241a-4c8d-80e9-ce050a2d6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2,\"id\",\"inner\").explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3d4149-a6c7-4f62-80e3-9d8f544672fd",
   "metadata": {},
   "source": [
    "14.INVESTIGATE LAZY EVALUATION PROBLEM :\n",
    "          use actions to debug transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae9b67-8292-45d9-b6db-969f9107fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema() #Doesnt trigger job\n",
    "df.count() #Triggers full execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab10755e-0305-4448-815c-d203484866f4",
   "metadata": {},
   "source": [
    "15.TRACK FAILED JOBS:\n",
    "          best for tracking oom,file read,schema,mismatch errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34570c0-2128-439c-ae7d-96c433198c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "use Spark UI>Jobs tab>click failed job>review stderr logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
